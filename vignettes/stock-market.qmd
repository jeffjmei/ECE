---
title: Stock Market
output: html_document
author: Jeffrey Mei
date: 2025-08-19
server: shiny
---

```{R, message=FALSE, warning=FALSE}
#| echo: false 
#| context: setup

# Load Packages
devtools::load_all("~/Documents/Research/Code/ECE")
library(tidyverse)
library(changepoint)
library(tidyquant)
```

# Stock Market Data
Here, we compare the closing cost of `MSFT`, `META`, and `NVDA` stocks. 

```{R}
dateRangeInput("daterange", "Choose dates:",
               start = "2020-01-01",
               end   = "2025-05-01",
               min   = "2010-01-01",
               max   = Sys.Date())
plotOutput("stockPlot")
```

```{R}
#| context: server
output$stockPlot <- renderPlot({
  raw_data <- tidyquant::tq_get(
    c("MSFT", "META", "NVDA"),
    from = input$daterange[1],
    to   = input$daterange[2]
  )

  data <- raw_data %>%
    dplyr::select(symbol, date, close) %>%
    pivot_wider(
      names_from = symbol,
      values_from = close
    ) %>%
    drop_na() %>%
    rename(
      index = date,
      X1 = MSFT,
      X2 = META,
      X3 = NVDA
    ) %>%
    mutate(
      X1_mean = segment_mean(X1),
      X2_mean = segment_mean(X2),
      X3_mean = segment_mean(X3)
    )

  plot(data$index, data$X1, type = "l", col = "blue",
       ylim = range(data$X1, data$X2, data$X3),
       xlab = "Date", ylab = "Price")
  lines(data$index, data$X2, col = "red")
  lines(data$index, data$X3, col = "darkgreen")
  lines(data$index, data$X1_mean, col = "blue", lty = 2)
  lines(data$index, data$X2_mean, col = "red", lty = 2)
  lines(data$index, data$X3_mean, col = "darkgreen", lty = 2)
})
```

```{R}
#| echo: false
raw_data <- tq_get(
  c("MSFT", "META", "NVDA"), 
  from = "2020-01-01",
  to = "2025-05-01"
)
data <- raw_data %>%
  dplyr::select(symbol, date, close) %>%
  pivot_wider(
    names_from = symbol,
    values_from = close
  ) %>%
  drop_na() %>%
  # rename
  rename(
    index = date,
    X1 = MSFT,
    X2 = META,
    X3 = NVDA
  ) %>%
  #  mutate(
  #    X1 = log(X1),
  #    X2 = log(X2),
  #    X3 = log(X3)
  #  ) %>% 
  # get piecewise constant means
  mutate(
    X1_mean = segment_mean(X1),
    X2_mean = segment_mean(X2),
    X3_mean = segment_mean(X3)
  )
```

Under the standard segmentation method -- i.e., PELT with BIC penalty with `minseglen = 2`, we get the following mean-segmentation. It is difficult to see what PELT determines as the best mean-segmentation, because the breaks are so frequent. This can be adjusted by utilizing a manual penalty or changing the `minseglen`.

```{R}
#| echo: false
# Plot
ggplot(data) +
  geom_line(aes(x = index, y = X1), color = "red") +
  geom_line(aes(x = index, y = X2), color = "blue") +
  geom_line(aes(x = index, y = X3), color = "green") +
  geom_line(aes(x = index, y = X1_mean), color = "red") +
  geom_line(aes(x = index, y = X2_mean), color = "blue") +
  geom_line(aes(x = index, y = X3_mean), color = "green") +
  labs(
    title = "Change Point Segmentation",
    x = "",
    y = ""
  ) +
  theme_minimal()
```

```{R}
#| echo: false

# Check Fit
ggplot(data) +
  geom_hline(yintercept = 0) +
  geom_line(aes(x = index, y = X1 - X1_mean), color = "red") +
  geom_line(aes(x = index, y = X2 - X2_mean), color = "blue") +
  geom_line(aes(x = index, y = X3 - X3_mean), color = "green") +
  labs(
    title = "Segmentation Residuals",
    x = "",
    y = ""
  ) +
  theme_minimal()
```

We see a little bit of heteroscedasticity. Whatever variance/covariance we estimate, it will be the average across this time interval.

```{R}
#| include: false
# Bivariate Analysis
ggplot(data) + 
  geom_point(aes(x=X1 - X1_mean, y = X2 - X2_mean)) + 
  geom_hline(yintercept=0) + 
  geom_vline(xintercept=0) + 
  theme_minimal()

ggplot(data) + 
  geom_point(aes(x=X1 - X1_mean, y = X3 - X3_mean)) + 
  geom_hline(yintercept=0) + 
  geom_vline(xintercept=0) + 
  theme_minimal()
```


```{R, echo=FALSE}
#| echo: false
# Check Autocorrelation
X1 <- data$X1
X2 <- data$X2
X3 <- data$X3
X1_mean <- data$X1_mean
X2_mean <- data$X2_mean
X3_mean <- data$X3_mean
X1_demeaned <- X1 - X1_mean
X2_demeaned <- X2 - X2_mean
X3_demeaned <- X3 - X3_mean

par(mfrow = c(1, 3))
acf(X1_demeaned)
acf(X2_demeaned)
acf(X3_demeaned)
par(mfrow = c(1, 1))
```

Given this segmentation, it looks like there's still some auto-correlation. It's hard to claim this is due to inadequate fitting since it the fit is already so tight. This intrinsic auto-correlation deflates the estimated correlation. This means our estimated correlation will be deflated from the true value. 

```{R}
#| echo: false
# =================
# Compare Estimates
# =================

# calculate correlation
ece_cov <- matrix(c(
  equiv.cov(X1, X1),
  equiv.cov(X1, X2),
  equiv.cov(X1, X3),
  equiv.cov(X2, X1),
  equiv.cov(X2, X2),
  equiv.cov(X2, X3),
  equiv.cov(X3, X1),
  equiv.cov(X3, X2),
  equiv.cov(X3, X3)
), nrow = 3)

colnames(ece_cov) <- c("MSFT", "META", "NVDA")
rownames(ece_cov) <- c("MSFT", "META", "NVDA")
```


```{R}
#| echo: false

ece_cor <- matrix(c(
  ece.test(X1, X1)$estimate,
  ece.test(X1, X2)$estimate,
  ece.test(X1, X3)$estimate,
  ece.test(X2, X1)$estimate,
  ece.test(X2, X2)$estimate,
  ece.test(X2, X3)$estimate,
  ece.test(X3, X1)$estimate,
  ece.test(X3, X2)$estimate,
  ece.test(X3, X3)$estimate
), nrow = 3)

colnames(ece_cor) <- c("MSFT", "META", "NVDA")
rownames(ece_cor) <- c("MSFT", "META", "NVDA")
```


```{R}
#| echo: false 

# Pearson Covariance
demean_cov <- cov(cbind(X1 - X1_mean, X2 - X2_mean, X3 - X3_mean))
colnames(demean_cov) <- c("MSFT", "META", "NVDA")
rownames(demean_cov) <- c("MSFT", "META", "NVDA")

print("Demean Covariance")
print(demean_cov)
print("ECE Covariance")
print(ece_cov)
```

This shows that there is very strong correlation between MSFT and META, but only a moderate correlation between MSFT and NVDA.

```{R}
#| echo: false 

# Pearson Correlation
demean_cor <- cor(cbind(X1 - X1_mean, X2 - X2_mean, X3 - X3_mean))
colnames(demean_cor) <- c("MSFT", "META", "NVDA")
rownames(demean_cor) <- c("MSFT", "META", "NVDA")

print("Demean Correlation")
print(demean_cor)
print("ECE Correlation")
print(ece_cor)
```

Here, the correlations are much more modest after mean-segmentation. 

# Conclusions
The way `MSFT`, `META`, and `NVDA` were chosen is because we additionally compare `AAPL`, `GOOG`, and `AMZN`, but they yielded nonsensical results -- the correlation coefficient would be greater than 1 if it could be calculated at all. This is a major problem in finding applications for ECE. Many of the time series result in NA values. This is a huge detriment of the technique. 

Moreover, the estimate is extremely unstable. Try varying the end-date for the data-collection. You'll find the estimates vary widely, and often times you'll find NA values. 


## More
How do we determine which of the analyses are more accurate? Simulations should give a hint as to the behavior between the two 

Are we supposed to be using log-returns instead? 

Can you find the penalty value that minimizes the difference between ECE estimate and segmented-mean estimate? 



```{R}
#| echo: false
get_weights <- function(L){
    K = sum((1:L)^2)
    S = sum(1:L)
    norm = L * K - S^2
    a = sapply(1:L, function(k) K - k * S)  / norm
    return(a)
}

Ak <- function(n, k){
    (1/n) * (diag(n) - 0.5 * (Ck(n, k) + t(Ck(n, k))))
}

Ck <- function(n, k){
    if(k == 0){
        return(diag(1, n))
    }
    rbind(diag(1, n)[-(1:k),], 
          diag(1, n)[1:k,])
}

b_to_A <- function(b, n, L){
    A = sapply(1:L, function(k) b[k] * Ak(n, k), simplify=F)
    return(Reduce('+', A))
}

est <- function(X, L=2){
    n = nrow(X)
    b = get_weights(L)
    A = b_to_A(b, n, L)
    return(t(X) %*% A %*% X)
}

est(cbind(X1, X2, X3), L=2)

```


