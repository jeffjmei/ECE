---
title: Stock Market
output: html_document
author: Jeffrey Mei
date: 2025-08-19
server: shiny
---

```{R}
#| echo: false 
#| context: setup
#| message: false
#| warning: false

# Load Packages
devtools::load_all("~/Documents/Research/Code/ECE")
library(tidyverse)
library(changepoint)
library(tidyquant)
```

# Stock Market Data
Here, we compare the closing cost of `MSFT`, `META`, and `NVDA` stocks. 

Under the standard segmentation method -- i.e., PELT with BIC penalty with `minseglen = 2`, we get the following mean-segmentation. It is difficult to see what PELT determines as the best mean-segmentation, because the breaks are so frequent. This can be adjusted by utilizing a manual penalty or changing the `minseglen`.

We load data from the S&P 500 from [Cam Nugent on Kaggle](https://www.kaggle.com/datasets/camnugent/sandp500/data). This is the dataset that is used from the GeomCP paper.

Following that paper, we take the log-returns. That is, we take the log of the values in each time series, and then we take the first difference. 

::: columns

::: {.column width=50%}
```{R}
textInput("ticker1", "Ticker 1:", value = "MSFT")
textInput("ticker2", "Ticker 2:", value = "META")
textInput("ticker3", "Ticker 3:", value = "NVDA")
```
::: 

::: {.column width=50%}
```{R}
dateRangeInput("daterange", "Choose dates:",
               start = "2015-01-01",
               end   = "2016-12-01",
               min   = "2010-01-01",
               max   = Sys.Date())
```
::: 
::: 


```{R}
#| context: server

df_reactive <- reactive({
  tickers <- c(input$ticker1, input$ticker2, input$ticker3)
  tq_get(tickers,
         from = input$daterange[1],
         to   = input$daterange[2]) %>%
    dplyr::select(symbol, date, close) %>%
    pivot_wider(names_from = symbol, values_from = close) %>%
    rename(
      index = date,
      X1 = !!sym(input$ticker1),
      X2 = !!sym(input$ticker2),
      X3 = !!sym(input$ticker3)
    ) %>%
    mutate(
      X1 = log(X1) - log(dplyr::lag(X1)),
      X2 = log(X2) - log(dplyr::lag(X2)),
      X3 = log(X3) - log(dplyr::lag(X3))
    ) %>%
    drop_na() %>%
    mutate(
      X1_mean = segment_mean(X1),
      X2_mean = segment_mean(X2),
      X3_mean = segment_mean(X3)
    )
})

#| context: server
output$stockPlot <- renderPlot({
  df <- df_reactive()
  ggplot(df, aes(x=index)) +
    geom_line(aes(y=X1), color="red") +
    geom_line(aes(y=X2), color="blue") +
    geom_line(aes(y=X3), color="green") +
    geom_line(aes(y=X1_mean), color="red", linetype="dashed") +
    geom_line(aes(y=X2_mean), color="blue", linetype="dashed") +
    geom_line(aes(y=X3_mean), color="green", linetype="dashed") +
    labs(title="Change Point Segmentation", x="", y="") +
    theme_minimal()
})

output$residualPlot <- renderPlot({
  df <- df_reactive()
  ggplot(df, aes(x=index)) +
    geom_hline(yintercept=0) +
    geom_line(aes(y=X1 - X1_mean), color="red") +
    geom_line(aes(y=X2 - X2_mean), color="blue") +
    geom_line(aes(y=X3 - X3_mean), color="green") +
    labs(title="Segmentation Residuals", x="", y="") +
    theme_minimal()
})

output$acfPlots <- renderPlot({
  df <- df_reactive()
  par(mfrow=c(1,3))
  acf(df$X1 - df$X1_mean, main="X1")
  acf(df$X2 - df$X2_mean, main="X2")
  acf(df$X3 - df$X3_mean, main="X3")
  par(mfrow=c(1,1))
})

#| context: server
output$ece_cov <- renderPrint({
  df <- df_reactive()
  demean_cov <- cov(cbind(
    df$X1 - df$X1_mean,
    df$X2 - df$X2_mean,
    df$X3 - df$X3_mean
  ))
  colnames(demean_cov) <- rownames(demean_cov) <- c("X1","X2","X3")

  ece_cov <- matrix(c(
    equiv.cov(df$X1, df$X1),
    equiv.cov(df$X1, df$X2),
    equiv.cov(df$X1, df$X3),
    equiv.cov(df$X2, df$X1),
    equiv.cov(df$X2, df$X2),
    equiv.cov(df$X2, df$X3),
    equiv.cov(df$X3, df$X1),
    equiv.cov(df$X3, df$X2),
    equiv.cov(df$X3, df$X3)
  ), nrow=3, byrow=TRUE)
  colnames(ece_cov) <- rownames(ece_cov) <- c("X1","X2","X3")

  cat("Demean Covariance:\n")
  print(round(demean_cov, 3))
  cat("\nECE Covariance:\n")
  print(round(ece_cov, 3))
})

output$ece_cor <- renderPrint({
  df <- df_reactive()
  demean_cor <- cor(cbind(
    df$X1 - df$X1_mean,
    df$X2 - df$X2_mean,
    df$X3 - df$X3_mean
  ))
  colnames(demean_cor) <- rownames(demean_cor) <- c("X1","X2","X3")

  ece_cor <- matrix(c(
    ece.test(df$X1, df$X1)$estimate,
    ece.test(df$X1, df$X2)$estimate,
    ece.test(df$X1, df$X3)$estimate,
    ece.test(df$X2, df$X1)$estimate,
    ece.test(df$X2, df$X2)$estimate,
    ece.test(df$X2, df$X3)$estimate,
    ece.test(df$X3, df$X1)$estimate,
    ece.test(df$X3, df$X2)$estimate,
    ece.test(df$X3, df$X3)$estimate
  ), nrow=3, byrow=TRUE)
  colnames(ece_cor) <- rownames(ece_cor) <- c("X1","X2","X3")

  cat("Demean Correlation:\n")
  print(round(demean_cor, 3))
  cat("\nECE Correlation:\n")
  print(round(ece_cor, 3))
})

#| context: server
running_ece <- function(x, y, start = 1000) {
  n <- length(x)
  map_dbl(start:n, ~ ece.test(x[1:.x], y[1:.x])$estimate)
}

output$running_ece <- renderPlot({
  df <- df_reactive()
  X1_X2 <- running_ece(df$X1, df$X2, 0.9 * nrow(df))
  X1_X3 <- running_ece(df$X1, df$X3, 0.9 * nrow(df))
  X2_X3 <- running_ece(df$X2, df$X3, 0.9 * nrow(df))

  df <- cbind(index=70:length(df$X1), X1_X2, X1_X3, X2_X3)
  ggplot(data=df) + 
    geom_line(aes(x=index, y=X1_X2, color="X1 vs X2")) + 
    geom_line(aes(x=index, y=X1_X3, color="X1 vs X3")) + 
    geom_line(aes(x=index, y=X2_X3, color="X2 vs X3")) + 
    geom_hline(yintercept=-1) + 
    geom_hline(yintercept= 1) + 
    scale_color_manual(values = c(
      "X1 vs X2" = "red",
      "X1 vs X3" = "blue",
      "X2 vs X3" = "green"
    )) +
    ylab("Correlation") + 
    theme_minimal()
})
```

::: columns
::: {.column width=70%}

::: {.panel-tabset}

## Plot
```{R}
plotOutput("stockPlot")
```


## Residuals
```{R}
plotOutput("residualPlot")
```

We see a little bit of heteroscedasticity. Whatever variance/covariance we estimate, it will be the average across this time interval.

## ACF
```{R}
plotOutput("acfPlots")
```

Given this segmentation, it looks like there's still some auto-correlation. It's hard to claim this is due to inadequate fitting since it the fit is already so tight. This intrinsic auto-correlation deflates the estimated correlation. This means our estimated correlation will be deflated from the true value. 

## Stability
```{R}
#| panel: fill
plotOutput("running_ece")
```
:::
::: 


::: {.column width=30%}

::: {.panel-tabset}

## Correlation
```{R}
verbatimTextOutput("ece_cor")
```

## Covariance
```{R}
verbatimTextOutput("ece_cov")
```

:::
::: 

# Conclusions
The way `MSFT`, `META`, and `NVDA` were chosen is because we additionally compare `AAPL`, `GOOG`, and `AMZN`, but they yielded nonsensical results -- the correlation coefficient would be greater than 1 if it could be calculated at all. This is a major problem in finding applications for ECE. Many of the time series result in NA values. This is a huge detriment of the technique. 

Moreover, the estimate is extremely unstable. Try varying the end-date for the data-collection. You'll find the estimates vary widely, and often times you'll find NA values. 


# More
How do we determine which of the analyses are more accurate? Simulations should give a hint as to the behavior between the two 

Are we supposed to be using log-returns instead? 

Can you find the penalty value that minimizes the difference between ECE estimate and segmented-mean estimate? 

To evaluate the stability, let's plot correlation of `MSFT-META`, `MSFT-NVDA`, AND `META-NVDA` over time. 




