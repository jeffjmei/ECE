---
title: FRED Analysis
author: Jeffrey Mei
output: html_document
date: 2025-09-22
server: shiny
---

[This](https://arxiv.org/pdf/2207.12453) paper evaluates some data from the Federal Reserve Economic Database (FRED). Specifically, the data comes from [here](https://research.stlouisfed.org/econ/mccracken/fred-databases), but it can be easily cleaned from the `fbi` package on [github](https://github.com/cykbennie/fbi). The description of each of the macroeconomic indicators can be found in the appendix of the [working paper](https://s3.amazonaws.com/real.stlouisfed.org/wp/2015/2015-012.pdf). 


```{R}
#| context: setup
#| warning: false
#| echo: false

# Install Package
# devtools::install_github("cykbennie/fbi")
library(fbi)
library(tidyverse)
library(patchwork)
devtools::load_all("~/Documents/Research/Code/ECE")
```


```{R}
#| echo: false
#| message: false
#| warning: false
#| include: false
#directory <- "~/Documents/Research/Code/ECE/data/FRED/"
#df <- list.files(
#  path = directory,
#  pattern = "\\.csv$",   # only CSV files
#  full.names = TRUE
#) %>%
#  map_dfr(read_csv, .id = "file")  # adds a column showing source file

```

```{R}
# Load Data
#filepath <- paste0(directory, "2023-08.csv")
#data <- fredmd(filepath, date_start = NULL, date_end = NULL, transform = TRUE)
#N <- ncol(data)
#
## Remove Outliers
#data_clean <- rm_outliers.fredmd(data)
#
## Filter Time Period
#df <- data_clean %>% 
#  as_tibble() %>% 
#  filter(
#    date >= as.Date("2000-01-01"),
#    date <= as.Date("2022-12-01")
#  )
  
  #nrow(df)
  #ncol(df)
```


```{R}
#| context: server
df_reactive <- reactive({

  # Load Data
  directory <- "~/Documents/Research/Code/ECE/data/FRED/"
  filepath <- paste0(directory, "2023-08.csv")

  # Apply Transformation
  if(input$data_format == "raw"){
    # Raw Data
    data <- read_csv(filepath) %>%
      slice(-1) %>% 
      rename(date = sasdate) %>%
      mutate(date = mdy(date))
  }else{
    data <- fredmd(
      filepath, 
      date_start = NULL, date_end = NULL, 
      transform = TRUE
    ) %>% rm_outliers.fredmd() 
  }

  # Filter Time Period
  df <- data %>% 
    as_tibble() %>% 
    filter(
      date >= as.Date(input$daterange[1]),
      date <= as.Date(input$daterange[2])
    ) %>% 
    dplyr::select(
      date,
      all_of(c(
        input$index1, 
        input$index2, 
        input$index3
      ))
    ) %>%
    mutate(across(-date, as.numeric)) %>% 
    rename(
      index = date, 
      X1 = !!sym(as.character(input$index1)),
      X2 = !!sym(as.character(input$index2)),
      X3 = !!sym(as.character(input$index3))
    ) %>% 
    drop_na() 

  # Apply Log-Transform
  if (!is.null(input$log_vars)) {
    df <- df %>%
      mutate(across(all_of(input$log_vars), ~ ifelse(. > 0, log(.), NA)))
  }
  df <- df %>%
    mutate(
      X1_mean = segment_mean(X1, penalty="Manual", pen.value=input$penalty),
      X2_mean = segment_mean(X2, penalty="Manual", pen.value=input$penalty),
      X3_mean = segment_mean(X3, penalty="Manual", pen.value=input$penalty)
    )
    df

})

output$ts_plot <- renderPlot({
  df <- df_reactive()
  ggplot(df, aes(x=index)) +
    geom_line(aes(y=X1), color="red") +
    geom_line(aes(y=X2), color="blue") +
    geom_line(aes(y=X3), color="green") +
    geom_line(aes(y=X1_mean), color="red", linetype="dashed") +
    geom_line(aes(y=X2_mean), color="blue", linetype="dashed") +
    geom_line(aes(y=X3_mean), color="green", linetype="dashed") +
    labs(title="Change Point Segmentation", x="", y="") +
    theme_minimal()
})

output$residualPlot <- renderPlot({
  df <- df_reactive()
  ggplot(df, aes(x=index)) +
    geom_hline(yintercept=0) +
    geom_line(aes(y=X1 - X1_mean), color="red") +
    geom_line(aes(y=X2 - X2_mean), color="blue") +
    geom_line(aes(y=X3 - X3_mean), color="green") +
    labs(title="Segmentation Residuals", x="", y="") +
    theme_minimal()
})

output$bivariate <- renderPlot({
  df <- df_reactive()
  p12 <- ggplot(df, aes(x=X1 - X1_mean, y=X2 - X2_mean)) +
    geom_hline(yintercept=0) +
    geom_vline(xintercept=0) +
    geom_point() + 
    geom_smooth(method="lm", se=FALSE, color="red") +
    labs(title="X1 vs X2", x="X1", y="X2") + 
    theme_minimal()

  p13 <- ggplot(df, aes(x=X1 - X1_mean, y=X3 - X3_mean)) +
    geom_hline(yintercept=0) +
    geom_vline(xintercept=0) +
    geom_point() + 
    geom_smooth(method="lm", se=FALSE, color="red") +
    labs(title="X1 vs X3", x="X1", y="X3") + 
    theme_minimal()

  p23 <- ggplot(df, aes(x=X2 - X2_mean, y=X3 - X3_mean)) +
    geom_hline(yintercept=0) +
    geom_vline(xintercept=0) +
    geom_point() + 
    geom_smooth(method="lm", se=FALSE, color="red") +
    labs(title="X2 vs X3", x="X2", y="X3") + 
    theme_minimal()

  (p12 | p13) /
    (plot_spacer() | p23)
})

output$detrendPlot <- renderPlot({
  df <- df_reactive()

  # Select the columns to detrend
  X <- df %>% dplyr::select(X1, X2, X3)

  # Detrend by subtracting the rotated (lagged) version
  X_detrended <- as.matrix(X) - rotate(as.matrix(X))

  # Bind the detrended data back with the index
  df_detrended <- dplyr::bind_cols(df["index"], X_detrended)

  ggplot(df_detrended, aes(x = index)) +
    geom_hline(yintercept = 0) +
    geom_line(aes(y = X1), color = "red") +
    geom_line(aes(y = X2), color = "blue") +
    geom_line(aes(y = X3), color = "green") + 
    labs(title="Detrended Data", x="", y="") +
    theme_minimal()
})

output$acfPlots <- renderPlot({
  df <- df_reactive()
  par(mfrow=c(1,3))
  acf(df$X1 - df$X1_mean, main="X1")
  acf(df$X2 - df$X2_mean, main="X2")
  acf(df$X3 - df$X3_mean, main="X3")
  par(mfrow=c(1,1))
})

running_ece <- function(x, y, start = 1000) {
  n <- length(x)
  map_dbl(start:n, ~ ece.test(x[1:.x], y[1:.x])$estimate)
}

output$running_ece <- renderPlot({
  df <- df_reactive()
  running_ece_start <- round(0.9 * nrow(df))
  X1_X2 <- running_ece(df$X1, df$X2, start=running_ece_start)
  X1_X3 <- running_ece(df$X1, df$X3, start=running_ece_start)
  X2_X3 <- running_ece(df$X2, df$X3, start=running_ece_start)

  df <- cbind(index=70:length(df$X1), X1_X2, X1_X3, X2_X3)
  ggplot(data=df) + 
    geom_line(aes(x=index, y=X1_X2, color="X1 vs X2")) + 
    geom_line(aes(x=index, y=X1_X3, color="X1 vs X3")) + 
    geom_line(aes(x=index, y=X2_X3, color="X2 vs X3")) + 
    geom_hline(yintercept=-1) + 
    geom_hline(yintercept= 1) + 
    scale_color_manual(values = c(
      "X1 vs X2" = "red",
      "X1 vs X3" = "blue",
      "X2 vs X3" = "green"
    )) +
    ylab("Correlation") + 
    theme_minimal()
})

output$ece_regression <- renderPlot({
  df <- df_reactive()
  plot.ece(data.frame(X1=df$X1, X2=df$X2))
})

output$cov <- renderPrint({
  df <- df_reactive()
  demean_cov <- cov(cbind(
    df$X1 - df$X1_mean,
    df$X2 - df$X2_mean,
    df$X3 - df$X3_mean
  ))
  colnames(demean_cov) <- rownames(demean_cov) <- c("X1","X2","X3")

  X <- df %>% 
    dplyr::select(X1, X2, X3) %>% 
    as.matrix()

  # Pearson
  pearson_cov <- cov(X)    
  colnames(pearson_cov) <- rownames(pearson_cov) <- c("X1","X2","X3")

  # Detrend
  detrend_cov <- cov((X - rotate(X))[-nrow(X),]) / 2    
  colnames(detrend_cov) <- rownames(detrend_cov) <- c("X1","X2","X3")

  # ECE
  ece_cov <- equiv.cov(as.matrix(X))
  colnames(ece_cov) <- rownames(ece_cov) <- c("X1","X2","X3")

  cat("\nPearson Covariance:\n")
  print(round(pearson_cov, 3))
  cat("\nDetrend Covariance:\n")
  print(round(detrend_cov, 3))
  cat("\nDemean Covariance:\n")
  print(round(demean_cov, 3))
  cat("\nECE Covariance:\n")
  print(round(ece_cov, 3))
})

output$cor <- renderPrint({
  df <- df_reactive()
  demean_cor <- cor(cbind(
    df$X1 - df$X1_mean,
    df$X2 - df$X2_mean,
    df$X3 - df$X3_mean
  ))
  colnames(demean_cor) <- rownames(demean_cor) <- c("X1","X2","X3")

  X <- df %>% 
    dplyr::select(X1, X2, X3) %>%
    as.matrix()

  # Pearson
  pearson_cor <- cor(X)    
  colnames(pearson_cor) <- rownames(pearson_cor) <- c("X1","X2","X3")

  # Detrend
  detrend_cor <- cov2cor(cov((X - rotate(X))[-nrow(X),]) / 2)
  colnames(detrend_cor) <- rownames(detrend_cor) <- c("X1","X2","X3")

  # ECE
  ece_cor <- cov2cor(equiv.cov(as.matrix(X)))
  colnames(ece_cor) <- rownames(ece_cor) <- c("X1","X2","X3")

  cat("\nPearson Correlation:\n")
  print(round(pearson_cor, 3))
  cat("\nDetrend Correlation:\n")
  print(round(detrend_cor, 3))
  cat("\nDemean Correlation:\n")
  print(round(demean_cor, 3))
  cat("\nECE Correlation:\n")
  print(round(ece_cor, 3))
})

output$pvals <- renderPrint({

  df <- df_reactive()
  X <- df %>% 
    dplyr::select(X1, X2, X3) %>% 
    as.matrix()

  # Pearson
  pearson_pval <- cor_pval(X)

  # Detrend
  detrend_pval <- cor_pval(detrend(X))

  # ECE 
  ece_pval <- ece_pval(X)

  # Segmentation
  demean_pval <- cor_pval(cbind(
    df$X1 - df$X1_mean,
    df$X2 - df$X2_mean,
    df$X3 - df$X3_mean
  ))

  # Output Results
  cat("\nPearson P-Values:\n")
  print(round(pearson_pval, 3))
  cat("\nDetrend P-Values:\n")
  print(round(detrend_pval, 3))
  cat("\nDemean P-Values:\n")
  print(round(demean_pval, 3))
  cat("\nECE P-Values:\n")
  print(round(ece_pval, 3))

})

```

::: columns

::: {.column width=50%}
```{R}
choices <- c(
  "RPI", "W875RX1", "DPCERA3M086SBEA",
  "CMRMTSPLx", "RETAILx", "INDPRO", "IPFPNSS",
  "IPFINAL", "IPCONGD", "IPDCONGD", "IPNCONGD",
  "IPBUSEQ", "IPMAT", "IPDMAT", "IPNMAT",
  "IPMANSICS", "IPB51222S", "IPFUELS", "CUMFNS",
  "HWI", "HWIURATIO", "CLF16OV", "CE16OV",
  "UNRATE", "UEMPMEAN", "UEMPLT5", "UEMP5TO14",
  "UEMP15OV", "UEMP15T26", "UEMP27OV", "CLAIMSx",
  "PAYEMS", "USGOOD", "CES1021000001", "USCONS",
  "MANEMP", "DMANEMP", "NDMANEMP", "SRVPRD",
  "USTPU", "USWTRADE", "USTRADE", "USFIRE",
  "USGOVT", "CES0600000007", "AWOTMAN", "AWHMAN",
  "HOUST", "HOUSTNE", "HOUSTMW", "HOUSTS",
  "HOUSTW", "PERMIT", "PERMITNE", "PERMITMW",
  "PERMITS", "PERMITW", "ACOGNO", "AMDMNOx",
  "ANDENOx", "AMDMUOx", "BUSINVx", "ISRATIx",
  "M1SL", "M2SL", "M2REAL", "BOGMBASE",
  "TOTRESNS", "NONBORRES", "BUSLOANS", "REALLN",
  "NONREVSL", "CONSPI", "S&P 500", "S&P div yield",
  "S&P PE ratio", "FEDFUNDS", "CP3Mx", "TB3MS",
  "TB6MS", "GS1", "GS5", "GS10",
  "AAA", "BAA", "COMPAPFFx", "TB3SMFFM",
  "TB6SMFFM", "T1YFFM", "T5YFFM", "T10YFFM",
  "AAAFFM", "BAAFFM", "TWEXAFEGSMTHx", "EXSZUSx",
  "EXJPUSx", "EXUSUKx", "EXCAUSx", "WPSFD49207",
  "WPSFD49502", "WPSID61", "WPSID62", "OILPRICEx",
  "PPICMM", "CPIAUCSL", "CPIAPPSL", "CPITRNSL",
  "CPIMEDSL", "CUSR0000SAC", "CUSR0000SAD", "CUSR0000SAS",
  "CPIULFSL", "CUSR0000SA0L2", "CUSR0000SA0L5", "PCEPI",
  "DDURRG3M086SBEA", "DNDGRG3M086SBEA", "DSERRG3M086SBEA", "CES0600000008",
  "CES2000000008", "CES3000000008", "UMCSENTx", "DTCOLNVHFNM",
  "DTCTHFNM", "INVEST", "VIXCLSx"
)

selectInput(
  "index1", "Index 1:",
  choices = choices,
  selected = "UNRATE"
)

selectInput(
  "index2", "Index 2:",
  choices = choices,
  selected = "AAA"
)

selectInput(
  "index3", "Index 3:",
  choices = choices,
  selected = "FEDFUNDS"
)

textInput("penalty", "Segmentation Penalty:", value = 0.01)
```
::: 

::: {.column width=50%}
```{R}
dateRangeInput("daterange", "Choose dates:",
               start = "2000-01-01",
               end   = "2022-12-01",
               min   = "2000-01-01",
               max   = "2024-01-01")

radioButtons(
  "data_format", "Data Format:",
  choices = c("Raw" = "raw", "Transformed" = "transformed"),
  selected = "transformed"
)

checkboxGroupInput(
  "log_vars", "Apply log transformation to:",
  choices = c("X1", "X2", "X3"),
  selected = NULL
)

```
::: 

::: 




::: {.panel-tabset}

## Plot
```{R}
#| panel: fill
plotOutput("ts_plot")
```


## Residuals
```{R}
#| panel: fill
plotOutput("residualPlot")
plotOutput("bivariate")
```

## Detrend
```{R}
#| panel: fill
plotOutput("detrendPlot")
```

## ACF
```{R}
#| panel: fill
plotOutput("acfPlots")
```

## Stability
```{R}
#| panel: fill
plotOutput("running_ece")
```

## ECE-Regression
```{R}
#| panel: fill
plotOutput("ece_regression")
```

:::



::: {.panel-tabset}

## Correlation
::: columns

::: {.column width="50%"}

```{R}
verbatimTextOutput("cor")
```
:::

::: {.column width="50%"}
```{R}
verbatimTextOutput("pvals")
```
:::

:::

## Covariance
```{R}
verbatimTextOutput("cov")
```

:::

--- 

# Notes

::: {.callout-note collapse="true"}
## Transformations
The FRED data comes with its own recommendations for the transformation. These transformations are designed to produce stationary series. The transformation codes can be found [here](https://www.tandfonline.com/doi/pdf/10.1080/07350015.2015.1086655):  
1. no transformation  
2. 1st difference  
3. 2nd difference  
4. log-transform  
5. log-first difference  
6. log-second difference  
7. 1st ratio  

Overall, while analyzing this dataset, I find that it runs into the same issue as stock data. ECE tends to estimate 0 variance, causing its correlation estimates to be extremely volatile. It is thus difficult to recommend its use in economic and financial applications. 

In terms of interpretation, we are estimating how much variation is unexplained by systematic effects. Systematic effects can generally be removed via first differencing. However, if there are many jumps, or large jumps, we will observe bias in the correlation estimate. 
::: 

::: {.callout-note collapse="true"}
## Change Point Are Ubiquitous Post-Transformation
The reason why we use these recommended transformations is because they are meant to make the distribution roughly stationary. However, as we see, these transformations may still contain some change points. 

Here are some examples:   
- `UNRATE`  
- `FEDFUNDS`  
- `AAA`  
- `RPI`  

In fact, these are ubiquitous. 
::: 

::: {.callout-note collapse="true"}
## Output and Income

- `IPMAT` (log-first-difference): IP Materials
- `IPMANSICS` (log-first-difference): IP Manufacturing
- `INDPRO` (log-first-difference): IP Index

Change points are more prevalent in this group. We see ECE produce slightly deflated values relative to Pearson. However, the conclusions are ultimately the same. 

---

- `RPI` (log-1st-difference): real personal income
- `IPCONGD` (log-1st-difference): IP: consumer goods
- `INDPRO` (log-1st-difference): industrial production index

There are obvious signs of changes in the raw data, but there are no change points in the transformed data.

Simulations should be conducted to evaluate the performance on this kind of data. 

::: 

::: {.callout-note collapse="true"}
## Manufacturing
- `IPMANSICS`: IP: Manufacturing
- `MANEMP`: All Employees: Manufacturing
- `CES3000000008`: Avg Hourly Earnings: Manufacturing

There are some obvious jumps here. Unfortunately, there's no change in deciding which variables are correlated. 

::: 

::: {.callout-note collapse="true"}
## Housing 
- `HOUSTS` (log-transform): housing starts, south
- `HOUSTW` (log-transform): housing starts, west
- `HOUSTNE` (log-transform): housing starts, northeast

There's an obvious change point around 2008. As a consequence, Pearson produces high correlation, whereas ECE basically has no correlation. This is a case where the difference with Pearson is obvious.

The same can be said of 
- `PERMITS` (log-transform): new housing, south  
- `PERMITW` (log-transform): new housing, west  
- `PERMITNE` (log-transform): new housing, northeast  

::: 

::: {.callout-note collapse="true"}
## Money and Credit
- `FEDFUNDS` (1st-difference): effective federal funds rate
- `TB3MS` (1st-difference): 3 month treasury bill
- `TB6MS` (1st-difference): 6 month treasury bill

Here, even though we took the 1st difference, there are very obvious change points. This is a beautiful case. There is a difference between Pearson and this one. 

It shows that FEDFUNDS are uncorrelated with the other two series, but the two series are correlated with each other. This is interesting, because just by looking at it, all three series appear to have the same systematic effect. 

**Interpretation**: 


::: 

::: {.callout-note collapse="true"}
## Interest Rate and Exchange Rates
- `GS1` (1st-difference): 1 year treasury rate
- `GS10` (1st-difference): 10 year treasury rate
- `FEDFUNDS` (1st-difference): effective federal funds rate

This is another good example. `FEDFUNDS` has change points, and this impacts the correlation with all the other cases. There is a difference with Pearson. It is very similar to the comparison with treasury bonds `TB3MS` and `TB6MS`. 
::: 

::: {.callout-note collapse="true"}
## Exchange Rates
- `EXSZUSx` (log-first-difference): Switzerland-US Foreign Exchange Rate
- `EXCAUSx` (log-first-difference): Canada-US Foreign Exchange Rate
- `EXJPUSx` (log-first-difference): Japan-US Foreign Exchange Rate
- `EXUSUKx` (log-first-difference): US-UK Foreign Exchange Rate

This is a good example, because it is difficult to identify the change points, but we still find there's a difference with Pearson.

I'm not convinced there _is_ a change point here. When we change the sensitivity of PELT, it mostly seems to follow the noise. 
::: 

::: {.callout-note collapse="true"}
## Interpretation 
Generally speaking, most of these indices use some first-differencing. What we observe is that despite these recommendations for making the data stationary, we still observe the impact of structural breaks within the transformed data. Consequently, we argue the correlation between series are exaggerated when using Pearson correlation. 

Why do these transformations fail to capture these change points? Consider the most common transformation: the first-difference of logs (lagged-log-ratios). They can represent periods of accelerated growth. 
::: 

::: {.callout-note collapse="true"}
## How is correlation between indices used in practice?  
Analysts often use correlation to measure co-variability of systematic effects. If two series vary together, then it is a sign that there is some systematic cause for their variation.

In contrast, ECE estimates the correlation after accounting for systematic effects. 
::: 

::: {.callout-note collapse="true"}
## Literature 
Source: [Pesaran (2004)](https://docs.iza.org/dp1240.pdf?utm_source=chatgpt.com)

In this paper, the author proposes a method for testing cross-sectional correlation. In their application, they apply a regression with response `log(per_capita_output)` on the intercept, a linear trend, $y_{t-1}$ and $y_{t-2}$ so the model is 
$$
y_{it} = \alpha_i + \beta_i t + \gamma_{1i} y_{i, t-1} + \gamma_{2i} y_{i, t-2} + u_{it}.
$$
We assume a simpler model where $\gamma_{1i} = \gamma_{2i} = 0$. 

The author notes that the empirical growth literature had been analyzing the data assuming cross-sectional independence despite acknowledging that it is not true. 

The author claims their method is robust to change points, but it is obvious that they may still be prevalent in the data even after a transformation. 

It's worth noting that the approach is a little different than what we're doing. They are ultimately doing regression, and a response variable must be selected. They also write in their introduction a good piece about how spatial dependence is conditional on the kind of spatial structure you choose. This may not be entirely obvious for cases where there are political ramifications as well. 

---

Source: [Gao et al. (2019)](https://arxiv.org/pdf/1904.06843)  

The authors aim to estimate high-dimensional cross-sectional correlation estimates. They note that it is unrealistic to expect no cross-sectional correlation, while acknowledging it is equally unrealistic to expect cross-sectional correlation among all components. They propose a **factor model** to model the latent structures. 

---

Source: [Rank-Based Tests of Cross-Sectional Dependence in Panel Data Models](https://www.sciencedirect.com/science/article/pii/S0167947320301614)

Note: This paper's introduction is superb. 

Cross-sectional correlation can represent a variety of things: interaction between cross-sectional units, unobservable common factors, or common shocks. This paper argues most papers in the literature rely on normality assumptions. In response, this paper introduces a non-parametric approach to solving the problem of estimating cross-sectional correlation. 

--- 

Source: [Forecasting using heterogeneous panels with cross-sectional dependence (2020)](https://www.sciencedirect.com/science/article/pii/S0169207019302687?utm_source=chatgpt.com)

::: 
